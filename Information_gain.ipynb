{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.random.bit_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
      "File \u001b[1;32md:\\conda\\envs\\py38\\lib\\site-packages\\sklearn\\__init__.py:83\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     80\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     86\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    130\u001b[0m     ]\n",
      "File \u001b[1;32md:\\conda\\envs\\py38\\lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester\n",
      "File \u001b[1;32md:\\conda\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n",
      "File \u001b[1;32md:\\conda\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\conda\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\validation.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_isfinite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FiniteStatus, cy_isfinite\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _object_dtype_isnan\n",
      "File \u001b[1;32md:\\conda\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\_array_api.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_array_api_dispatch\u001b[39m(array_api_dispatch):\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that array_api_compat is installed and NumPy version is compatible.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    array_api_compat follows NEP29, which has a higher minimum NumPy version than\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    scikit-learn.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\conda\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\fixes.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreadpoolctl\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n",
      "File \u001b[1;32md:\\conda\\envs\\py38\\lib\\site-packages\\scipy\\stats\\__init__.py:485\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    480\u001b[0m \n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    484\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 485\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32md:\\conda\\envs\\py38\\lib\\site-packages\\scipy\\stats\\_stats_py.py:46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributions\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _mstats_basic \u001b[38;5;28;01mas\u001b[39;00m mstats_basic\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_mstats_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[0;32m     49\u001b[0m                                    siegelslopes)\n",
      "File \u001b[1;32md:\\conda\\envs\\py38\\lib\\site-packages\\scipy\\stats\\distributions.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_distn_infrastructure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _continuous_distns\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _discrete_distns\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_continuous_distns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_levy_stable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m levy_stable\n",
      "File \u001b[1;32md:\\conda\\envs\\py38\\lib\\site-packages\\scipy\\stats\\_discrete_distns.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_distn_infrastructure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (rv_discrete, get_distribution_names,\n\u001b[0;32m     18\u001b[0m                                     _check_shape, _ShapeInfo)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_boost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_boost\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_biasedurn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_PyFishersNCHypergeometric,\n\u001b[0;32m     21\u001b[0m                         _PyWalleniusNCHypergeometric,\n\u001b[0;32m     22\u001b[0m                         _PyStochasticLib3)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_isintegral\u001b[39m(x):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mround(x)\n",
      "File \u001b[1;32m_biasedurn.pyx:1\u001b[0m, in \u001b[0;36minit scipy.stats._biasedurn\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy.random.bit_generator'"
     ]
    }
   ],
   "source": [
    "# 导入需要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \n\u001b[0;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv') \n",
    "\n",
    "X = data.iloc[:,1:-1]  \n",
    "y = data.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 信息熵\n",
    "def calc_entropy(y):\n",
    "    y = pd.Series(y) \n",
    "    counts = y.value_counts()\n",
    "    # #打印每个类别的数量\n",
    "    # print(counts)\n",
    "    ps = counts / len(y)\n",
    "    # print(ps)\n",
    "    return -ps.map(lambda p: p * np.log2(p)).sum()\n",
    "\n",
    "#处理连续值\n",
    "def calc_entropy2(y,spilt_point):\n",
    "    # print(y)\n",
    "    y = pd.Series(y)\n",
    "    y1=y[y<=spilt_point]\n",
    "    y2=y[y>spilt_point]    \n",
    "    ps1 = len(y1) / len(y)\n",
    "    ps2 = len(y2) / len(y)\n",
    "    return -(ps1*np.log2(ps1)+ps2*np.log2(ps2))\n",
    "\n",
    "\n",
    "# 计算条件熵\n",
    "def cond_entropy(X, y):\n",
    "    y = pd.Series(y)\n",
    "    h_total = 0\n",
    "    # print(X.unique(),len(X.unique()))\n",
    "    for x in X.unique():\n",
    "        mask = X.isin([x]) & ~X.isna() # 选出X中等于x的样本\n",
    "        h_x = calc_entropy(y[mask]) # 计算条件熵\n",
    "        p_x = mask.mean() # 计算p(x)\n",
    "        h_total += p_x * h_x\n",
    "        # print(x, p_x, h_x)\n",
    "    return h_total\n",
    "\n",
    "\n",
    "\n",
    "# #打印信息熵\n",
    "# print(\"信息熵：\",calc_entropy(y))\n",
    "# #打印条件熵\n",
    "# print(\"色泽：\",cond_entropy(X['色泽'], y))\n",
    "# print(\"根蒂：\",cond_entropy(X['根蒂'], y))\n",
    "# print(\"敲声：\",cond_entropy(X['敲声'], y))\n",
    "# print(\"纹理：\",cond_entropy(X['纹理'], y))\n",
    "# print(\"脐部：\",cond_entropy(X['脐部'], y))\n",
    "# print(\"触感：\",cond_entropy(X['触感'], y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对于连续值的处理\n",
    "def generate_split_points(data, attribute):\n",
    "    unique_values = set(data[attribute])  # 获取属性的唯一取值\n",
    "    split_points = []  # 存储候选分割点\n",
    "    for i in range(len(unique_values) - 1):\n",
    "        current_value = sorted(list(unique_values))[i]\n",
    "        next_value = sorted(list(unique_values))[i + 1]\n",
    "        split_point = (current_value + next_value) / 2  # 取相邻两个值的中点作为分割点\n",
    "        split_points.append(split_point)\n",
    "    return split_points\n",
    "\n",
    "# print(generate_split_points(X, '密度'))\n",
    "# print(generate_split_points(X, '含糖率'))\n",
    "\n",
    "#选出最优的分割点\n",
    "def choose_best_split_point(data, attribute, y):\n",
    "    split_points = generate_split_points(data, attribute)\n",
    "    min_entropy = 1\n",
    "    best_split_point = None\n",
    "    for split_point in split_points:\n",
    "        mask = data[attribute] <= split_point\n",
    "        h = cond_entropy(pd.Series(mask), y)\n",
    "        if h < min_entropy:\n",
    "            min_entropy = h\n",
    "            best_split_point = split_point\n",
    "    info_gain = calc_entropy(y) - min_entropy\n",
    "    return best_split_point, info_gain\n",
    "\n",
    "def choose_best_split_point2(data, attribute, y):\n",
    "    split_points = generate_split_points(data, attribute)\n",
    "    min_entropy = 1\n",
    "    best_split_point = None\n",
    "    for split_point in split_points:\n",
    "        mask = data[attribute] <= split_point\n",
    "        h = cond_entropy(pd.Series(mask), y)\n",
    "        if h < min_entropy:\n",
    "            min_entropy = h\n",
    "            best_split_point = split_point\n",
    "    info_gain = calc_entropy(y) - min_entropy\n",
    "    Info_gain_rate = info_gain / calc_entropy2(data[attribute],best_split_point)\n",
    "    return best_split_point, Info_gain_rate\n",
    "# print(choose_best_split_point(X, '密度', y))\n",
    "# print(choose_best_split_point(X, '含糖率', y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Info_gain(X, y):\n",
    "    return calc_entropy(y) - cond_entropy(X, y)\n",
    "\n",
    "#信息增益率\n",
    "def Info_gain_rate(X, y):\n",
    "    return Info_gain(X, y) / calc_entropy(X)\n",
    "\n",
    "\n",
    "# #打印每个特征的信息增益\n",
    "# print(\"色泽：\",Info_gain(X['色泽'], y))\n",
    "# print(\"根蒂：\",Info_gain(X['根蒂'], y))\n",
    "# print(\"敲声：\",Info_gain(X['敲声'], y))\n",
    "# print(\"纹理：\",Info_gain(X['纹理'], y))\n",
    "# print(\"脐部：\",Info_gain(X['脐部'], y))\n",
    "# print(\"触感：\",Info_gain(X['触感'], y))\n",
    "# #打印连续值信息最佳分割点以及信息增益\n",
    "# print(\"密度：\",choose_best_split_point(X, '密度', y))\n",
    "# print(\"含糖率：\",choose_best_split_point(X, '含糖率', y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义决策树节点\n",
    "label_count = 0 \n",
    "class DecisionTreeNode:\n",
    "    def __init__(self):\n",
    "        global label_count\n",
    "        self.attribute = None # 存储属性\n",
    "        self.split_point = None # 存储分割点\n",
    "        self.result = None # 存储叶节点的类别\n",
    "        self.children = [] # 存储子节点\n",
    "        self.branchvaule = None #分支的值\n",
    "        self.label = label_count #用于广度优先遍历\n",
    "        self.count = 0 #用于可视化所有节点\n",
    "        self.fake_result = None # 用于后剪枝,存储子节点中样本数最多的类别\n",
    "        \n",
    "    def add_count(self):#分支的值\n",
    "        self.count +=1    \n",
    "        \n",
    "    def set_branchvalue(self,branchvaule):#分支的值\n",
    "        self.branchvaule = branchvaule\n",
    "\n",
    "    def set_attribute(self, attribute):\n",
    "        self.attribute = attribute\n",
    "\n",
    "    def set_split_point(self, split_point):\n",
    "        self.split_point = split_point\n",
    "\n",
    "    def set_result(self, result):\n",
    "        self.result = result\n",
    "\n",
    "    def set_child(self, child):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def set_fake_result(self, fake_result):\n",
    "        self.fake_result = fake_result\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.result is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预剪枝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 计算准确率\n",
    "def accuracy(y_true, y_pred):\n",
    "    # print(y_true,y_pred)\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "\n",
    "#寻找最佳属性\n",
    "def find_best_attribute(X, y):\n",
    "    best_info_gain = -np.inf\n",
    "    best_attribute = None\n",
    "    best_split_point = None\n",
    "    for attribute in X.columns:\n",
    "        if X[attribute].dtype == np.float64 or X[attribute].dtype == np.int32:  # 为浮点或整数类型，处理连续属性\n",
    "            split_point, info_gain = choose_best_split_point(X, attribute, y)\n",
    "            if info_gain > best_info_gain:\n",
    "                best_info_gain = info_gain\n",
    "                best_attribute = attribute\n",
    "                best_split_point = split_point\n",
    "        else:  # 处理离散属性\n",
    "            info_gain = Info_gain(X[attribute], y)\n",
    "            if info_gain > best_info_gain:\n",
    "                best_info_gain = info_gain\n",
    "                best_attribute = attribute\n",
    "                best_split_point = None\n",
    "    # #找到后将该属性从X中删除\n",
    "    # X.drop(best_attribute,axis=1,inplace=True)\n",
    "    return best_attribute,best_split_point,best_info_gain\n",
    "\n",
    "#寻找最佳属性\n",
    "def find_best_attribute2(X, y):\n",
    "    best_info_gain = -np.inf\n",
    "    best_attribute = None\n",
    "    best_split_point = None\n",
    "    for attribute in X.columns:\n",
    "        if X[attribute].dtype == np.float64:  # 处理连续属性\n",
    "            split_point, info_gain = choose_best_split_point2(X, attribute, y)\n",
    "            if info_gain > best_info_gain:\n",
    "                best_info_gain = info_gain\n",
    "                best_attribute = attribute\n",
    "                best_split_point = split_point\n",
    "        else:  # 处理离散属性\n",
    "            info_gain = Info_gain_rate(X[attribute], y)\n",
    "            if info_gain > best_info_gain:\n",
    "                best_info_gain = info_gain\n",
    "                best_attribute = attribute\n",
    "                best_split_point = None\n",
    "    return best_attribute,best_split_point,best_info_gain\n",
    "\n",
    "#判断是否预剪枝\n",
    "def is_pre_pruning(X,y,X_valid, y_valid,best_attribute,best_split_point,node):\n",
    "    #预剪枝,验证集不为空且所有样本不属于同一类别\n",
    "    if X_valid is not None:\n",
    "        \n",
    "        #计算划分前的准确率\n",
    "        y_pred_pre = Counter(y).most_common(1)[0][0]\n",
    "        print(\"划分前的结果：\",y_pred_pre)\n",
    "        accuracy_pre = accuracy(y_valid, [y_pred_pre]*len(y_valid))\n",
    "        print(\"划分前的准确率：\",accuracy_pre)\n",
    "        #计算划分后的准确率\n",
    "        y_pred_post = []\n",
    "        #是否为连续值属性\n",
    "        if best_split_point is None:  \n",
    "            # 处理离散属性\n",
    "            for value in X[best_attribute].unique():\n",
    "                mask = X[best_attribute] == value\n",
    "                mask_valid = X_valid[best_attribute] == value\n",
    "                y_subset = y[mask]\n",
    "                #验证集中的样本\n",
    "                X_valid_subset = X_valid[mask_valid]\n",
    "                y_valid_subset = y_valid[mask_valid]\n",
    "                \n",
    "                # 统计子集中最多类别得出该分支的类别\n",
    "                y_pred_post_bool = Counter(y_subset).most_common(1)[0][0]\n",
    "                # 将最多类别重复len(y_subset)次添加到列表\n",
    "                y_pred_post += [y_pred_post_bool] * len(y_valid_subset)\n",
    "                \n",
    "            y_pred_post = np.array(y_pred_post)\n",
    "            print(y_pred_post)\n",
    "            accuracy_post = accuracy(y_valid, y_pred_post)\n",
    "            print(\"离散属性划分后的准确率：\",accuracy_post)\n",
    "        else: # 处理连续属性\n",
    "            mask = X[best_attribute] <= best_split_point\n",
    "            mask_valid = X_valid[best_attribute] <= best_split_point\n",
    "            # 选出X中小于等于best_split_point的样本划分子树\n",
    "            y_left = y[mask]\n",
    "            X_valid_left = X_valid[mask_valid]\n",
    "            y_valid_left = y_valid[mask_valid]\n",
    "            \n",
    "            \n",
    "            # 选出X中大于best_split_point的样本划分子树\n",
    "            y_right = y[~mask]\n",
    "            X_valid_right = X_valid[~mask_valid]\n",
    "            y_valid_right = y_valid[~mask_valid]\n",
    "            \n",
    "            # 统计左子集中最多类别\n",
    "            y_pred_post_left = Counter(y_left).most_common(1)[0][0]\n",
    "            # 将最多类别重复len(y_subset)次添加到列表\n",
    "            y_pred_post += [y_pred_post_left] * len(y_valid_left)\n",
    "            \n",
    "            # 统计左子集中最多类别\n",
    "            y_pred_post_right = Counter(y_right).most_common(1)[0][0]\n",
    "            # 将最多类别重复len(y_subset)次添加到列表\n",
    "            y_pred_post += [y_pred_post_right] * len(y_valid_right)\n",
    "            \n",
    "            \n",
    "            y_pred_post = np.array(y_pred_post)\n",
    "            print(y_pred_post)\n",
    "            accuracy_post = accuracy(y_valid, y_pred_post)\n",
    "            print(\"连续属性划分后的准确率：\",accuracy_post)\n",
    "        #如果划分后的准确率小于划分前的准确率，创建叶子节点，选择最多的类别作为结果\n",
    "        if accuracy_post <= accuracy_pre:\n",
    "            # print(\"剪枝\")\n",
    "            # node = DecisionTreeNode()\n",
    "            # node.set_result(y_pred_pre)\n",
    "            return True\n",
    "        else:\n",
    "            print(\"不剪枝\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"没有验证集，不剪枝\")\n",
    "        return False\n",
    "    \n",
    "# 决策树代码\n",
    "def build_decision_tree(X, y):\n",
    "    global label_count \n",
    "    if len(set(y)) == 1:  # 所有样本属于同一类别，创建叶子节点\n",
    "        leaf_node = DecisionTreeNode()\n",
    "        leaf_node.set_result(y.iloc[0])\n",
    "        return leaf_node\n",
    "    \n",
    "    if len(X.columns) == 0:  # 所有属性已经用完，创建叶子节点，选择最多的类别作为结果\n",
    "        leaf_node = DecisionTreeNode()\n",
    "        most_common_label = y.value_counts().idxmax()\n",
    "        leaf_node.set_result(most_common_label)\n",
    "        return leaf_node\n",
    "    \n",
    "    # 选取最佳划分属性\n",
    "    best_info_gain = -np.inf\n",
    "    best_attribute = None\n",
    "    best_split_point = None\n",
    "    best_attribute,best_split_point,best_info_gain = find_best_attribute(X, y)\n",
    "    if best_info_gain < 0:  # 信息增益小于0，创建叶子节点，选择最多的类别作为结果\n",
    "        leaf_node = DecisionTreeNode()\n",
    "        most_common_label = y.value_counts().idxmax()\n",
    "        leaf_node.set_result(most_common_label)\n",
    "        return leaf_node\n",
    "    \n",
    "    node = DecisionTreeNode()\n",
    "    node.set_attribute(best_attribute)\n",
    "    node.set_split_point(best_split_point)\n",
    "    node.add_count()\n",
    "    node.set_fake_result(y.value_counts().idxmax())\n",
    "    \n",
    "    if not node.is_leaf():\n",
    "        label_count += 1\n",
    "        \n",
    "    print(X,y)\n",
    "    print(\"当前最佳划分属性：\",node.attribute,\"当前标签值：\",node.label,\"当前假设结果：\",node.fake_result)\n",
    "    \n",
    "    # 递归构建子树\n",
    "    if best_split_point is None:  # 处理离散属性\n",
    "        for value in X[best_attribute].unique():\n",
    "            mask = X[best_attribute] == value# 选出X中等于value的样本划分子树\n",
    "            X_subset = X[mask]\n",
    "            y_subset = y[mask]\n",
    "            if len(X_subset) == 0:  # 无样本，创建叶子节点，选择最多的类别作为结果\n",
    "                leaf_node = DecisionTreeNode()\n",
    "                most_common_label = y.value_counts().idxmax()#返回最多的类别\n",
    "                leaf_node.set_result(most_common_label)#设置叶子节点的结果\n",
    "                leaf_node.set_branchvalue(value)\n",
    "                print(leaf_node.branchvaule,0)\n",
    "                node.set_child(leaf_node)#设置子树\n",
    "            else:\n",
    "                childnode = build_decision_tree(X_subset, y_subset)#递归创建左子树\n",
    "                childnode.set_branchvalue(value)\n",
    "                print(childnode.branchvaule,len(X_subset),\"是否为好瓜:\",childnode.result)\n",
    "                node.set_child(childnode)#递归创建子树\n",
    "    else:  # 处理连续属性\n",
    "            #for循环分别处理大于和小于两个子树\n",
    "        for num in range(2):\n",
    "            if num == 0:\n",
    "                #小于等于分割点的部分\n",
    "                mask = X[best_attribute] <= best_split_point\n",
    "                X_subset = X[mask]\n",
    "                y_subset = y[mask]\n",
    "                if len(X_subset) == 0:  # 无样本，创建叶子节点，选择最多的类别作为结果\n",
    "                    leaf_node = DecisionTreeNode()\n",
    "                    most_common_label = y.value_counts().idxmax()\n",
    "                    leaf_node.set_result(most_common_label)\n",
    "                    leaf_node.set_branchvalue('<='+str(best_split_point))\n",
    "                    # print(leaf_node.branchvaule)\n",
    "                    node.set_child(leaf_node)\n",
    "                    print('无样本，判断为',most_common_label)\n",
    "                else:\n",
    "                    print(node.attribute,'小于等于',node.split_point,'样本个数：',len(X_subset))\n",
    "                    childnode = build_decision_tree(X_subset, y_subset)\n",
    "                    childnode.set_branchvalue('<='+str(best_split_point))\n",
    "                    print(childnode.branchvaule,len(X_subset),\"是否为好瓜:\",childnode.result)\n",
    "                    node.set_child(childnode)\n",
    "            if num == 1:\n",
    "                #大于分割点的部分\n",
    "                mask = X[best_attribute] > best_split_point\n",
    "                X_subset = X[mask]\n",
    "                y_subset = y[mask]    \n",
    "                if len(X_subset) == 0:  # 无样本，创建叶子节点，选择最多的类别作为结果\n",
    "                    leaf_node = DecisionTreeNode()\n",
    "                    most_common_label = y.value_counts().idxmax()\n",
    "                    leaf_node.set_result(most_common_label)\n",
    "                    leaf_node.set_branchvalue('>'+str(best_split_point))\n",
    "                    print(leaf_node.branchvaule,leaf_node.result)\n",
    "                    node.set_child(leaf_node)\n",
    "                    print('无样本，判断为',most_common_label)\n",
    "                else:\n",
    "                    print(node.attribute,'大于',node.split_point,'样本个数：',len(X_subset))\n",
    "                    childnode = build_decision_tree(X_subset, y_subset)\n",
    "                    childnode.set_branchvalue('>'+str(best_split_point))\n",
    "                    print(childnode.branchvaule,len(X_subset),\"是否为好瓜:\",childnode.result)\n",
    "                    node.set_child(childnode)\n",
    "    return node\n",
    "\n",
    "#信息增益率决策树代码\n",
    "def build_decision_tree_rate(X, y):\n",
    "    global label_count \n",
    "    if len(set(y)) == 1:  # 所有样本属于同一类别，创建叶子节点\n",
    "        leaf_node = DecisionTreeNode()\n",
    "        leaf_node.set_result(y.iloc[0])\n",
    "        return leaf_node\n",
    "    \n",
    "    if len(X.columns) == 0:  # 所有属性已经用完，创建叶子节点，选择最多的类别作为结果\n",
    "        leaf_node = DecisionTreeNode()\n",
    "        most_common_label = y.value_counts().idxmax()\n",
    "        leaf_node.set_result(most_common_label)\n",
    "        return leaf_node\n",
    "    \n",
    "    # 选取最佳划分属性\n",
    "    best_info_gain_rate = -np.inf\n",
    "    best_attribute = None\n",
    "    best_split_point = None\n",
    "    best_attribute,best_split_point,best_info_gain_rate = find_best_attribute2(X, y)\n",
    "    if best_info_gain_rate < 0:  # 信息增益小于0，创建叶子节点，选择最多的类别作为结果\n",
    "        leaf_node = DecisionTreeNode()\n",
    "        most_common_label = y.value_counts().idxmax()\n",
    "        leaf_node.set_result(most_common_label)\n",
    "        return leaf_node\n",
    "    \n",
    "    node = DecisionTreeNode()\n",
    "    node.set_attribute(best_attribute)\n",
    "    node.set_split_point(best_split_point)\n",
    "    node.add_count()\n",
    "    node.set_fake_result(y.value_counts().idxmax())\n",
    "    \n",
    "    if not node.is_leaf():\n",
    "        label_count += 1\n",
    "        \n",
    "    print(X,y)\n",
    "    print(\"当前最佳划分属性：\",node.attribute,\"当前标签值：\",node.label,\"当前假设结果：\",node.fake_result)\n",
    "    \n",
    "    # 递归构建子树\n",
    "    if best_split_point is None:  # 处理离散属性\n",
    "        for value in X[best_attribute].unique():\n",
    "            mask = X[best_attribute] == value# 选出X中等于value的样本划分子树\n",
    "            X_subset = X[mask]\n",
    "            y_subset = y[mask]\n",
    "            if len(X_subset) == 0:  # 无样本，创建叶子节点，选择最多的类别作为结果\n",
    "                leaf_node = DecisionTreeNode()\n",
    "                most_common_label = y.value_counts().idxmax()#返回最多的类别\n",
    "                leaf_node.set_result(most_common_label)#设置叶子节点的结果\n",
    "                leaf_node.set_branchvalue(value)\n",
    "                print(leaf_node.branchvaule,0)\n",
    "                node.set_child(leaf_node)#设置子树\n",
    "            else:\n",
    "                childnode = build_decision_tree(X_subset, y_subset)#递归创建左子树\n",
    "                childnode.set_branchvalue(value)\n",
    "                print(childnode.branchvaule,len(X_subset),\"是否为好瓜:\",childnode.result)\n",
    "                node.set_child(childnode)#递归创建子树\n",
    "    else:  # 处理连续属性\n",
    "            #for循环分别处理大于和小于两个子树\n",
    "        for num in range(2):\n",
    "            if num == 0:\n",
    "                #小于等于分割点的部分\n",
    "                mask = X[best_attribute] <= best_split_point\n",
    "                X_subset = X[mask]\n",
    "                y_subset = y[mask]\n",
    "                if len(X_subset) == 0:  # 无样本，创建叶子节点，选择最多的类别作为结果\n",
    "                    leaf_node = DecisionTreeNode()\n",
    "                    most_common_label = y.value_counts().idxmax()\n",
    "                    leaf_node.set_result(most_common_label)\n",
    "                    leaf_node.set_branchvalue('<='+str(best_split_point))\n",
    "                    # print(leaf_node.branchvaule)\n",
    "                    node.set_child(leaf_node)\n",
    "                    print('无样本，判断为',most_common_label)\n",
    "                else:\n",
    "                    print(node.attribute,'小于等于',node.split_point,'样本个数：',len(X_subset))\n",
    "                    childnode = build_decision_tree(X_subset, y_subset)\n",
    "                    childnode.set_branchvalue('<='+str(best_split_point))\n",
    "                    print(childnode.branchvaule,len(X_subset),\"是否为好瓜:\",childnode.result)\n",
    "                    node.set_child(childnode)\n",
    "            if num == 1:\n",
    "                #大于分割点的部分\n",
    "                mask = X[best_attribute] > best_split_point\n",
    "                X_subset = X[mask]\n",
    "                y_subset = y[mask]    \n",
    "                if len(X_subset) == 0:  # 无样本，创建叶子节点，选择最多的类别作为结果\n",
    "                    leaf_node = DecisionTreeNode()\n",
    "                    most_common_label = y.value_counts().idxmax()\n",
    "                    leaf_node.set_result(most_common_label)\n",
    "                    leaf_node.set_branchvalue('>'+str(best_split_point))\n",
    "                    print(leaf_node.branchvaule,leaf_node.result)\n",
    "                    node.set_child(leaf_node)\n",
    "                    print('无样本，判断为',most_common_label)\n",
    "                else:\n",
    "                    print(node.attribute,'大于',node.split_point,'样本个数：',len(X_subset))\n",
    "                    childnode = build_decision_tree(X_subset, y_subset)\n",
    "                    childnode.set_branchvalue('>'+str(best_split_point))\n",
    "                    print(childnode.branchvaule,len(X_subset),\"是否为好瓜:\",childnode.result)\n",
    "                    node.set_child(childnode)\n",
    "    return node\n",
    "\n",
    "\n",
    "# 预剪枝决策树代码        \n",
    "def build_decision_tree_prun(X, y, X_valid, y_valid):\n",
    "    global label_count \n",
    "    if len(set(y)) == 1:  # 所有样本属于同一类别，创建叶子节点\n",
    "        leaf_node = DecisionTreeNode()\n",
    "        leaf_node.set_result(y.iloc[0])\n",
    "        return leaf_node\n",
    "    \n",
    "    if len(X.columns) == 0:  # 所有属性已经用完，创建叶子节点，选择最多的类别作为结果\n",
    "        leaf_node = DecisionTreeNode()\n",
    "        most_common_label = y.value_counts().idxmax()\n",
    "        leaf_node.set_result(most_common_label)\n",
    "        return leaf_node\n",
    "\n",
    "    \n",
    "    \n",
    "     # 选取最佳划分属性\n",
    "    best_info_gain = -np.inf\n",
    "    best_attribute = None\n",
    "    best_split_point = None\n",
    "    best_attribute,best_split_point,best_info_gain = find_best_attribute(X, y)\n",
    "    if best_info_gain < 0:  # 信息增益小于0，创建叶子节点，选择最多的类别作为结果\n",
    "        leaf_node = DecisionTreeNode()\n",
    "        most_common_label = y.value_counts().idxmax()\n",
    "        leaf_node.set_result(most_common_label)\n",
    "        return leaf_node\n",
    "\n",
    "    \n",
    "    \n",
    "    node = DecisionTreeNode()\n",
    "    node.set_attribute(best_attribute)\n",
    "    node.set_split_point(best_split_point)\n",
    "    node.add_count()\n",
    "    node.set_fake_result(y.value_counts().idxmax())\n",
    "\n",
    "    if not node.is_leaf():\n",
    "        label_count += 1\n",
    "    \n",
    "    \n",
    "    #预剪枝,验证集不为空且所有样本不属于同一类别\n",
    "    if(is_pre_pruning(X,y,X_valid, y_valid,best_attribute,best_split_point,node)):\n",
    "        #剪枝\n",
    "        print(\"剪枝\")\n",
    "        most_common_label = y.value_counts().idxmax()#返回最多的类别\n",
    "        node.set_result(most_common_label)#设置叶子节点的结果\n",
    "        return node\n",
    "    else:\n",
    "        #不剪枝\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # 递归构建子树\n",
    "    if best_split_point is None:  # 处理离散属性\n",
    "        for value in X[best_attribute].unique():\n",
    "            mask = X[best_attribute] == value# 选出X中等于value的样本划分子树\n",
    "            mask_valid = X_valid[best_attribute] == value\n",
    "            X_subset = X[mask]\n",
    "            y_subset = y[mask]\n",
    "            X_valid_subset = X_valid[mask_valid]\n",
    "            y_valid_subset = y_valid[mask_valid]\n",
    "            print(X_subset,y_subset)\n",
    "            if len(X_subset) == 0:  # 无样本，创建叶子节点，选择最多的类别作为结果\n",
    "                leaf_node = DecisionTreeNode()\n",
    "                most_common_label = y.value_counts().idxmax()#返回最多的类别\n",
    "                leaf_node.set_result(most_common_label)#设置叶子节点的结果\n",
    "                leaf_node.set_branchvalue(value)\n",
    "                node.set_child(leaf_node)#设置左子树\n",
    "            else:\n",
    "                print(X_valid_subset,y_valid_subset)\n",
    "                childnode = build_decision_tree_prun(X_subset, y_subset,X_valid_subset,y_valid_subset)#递归创建左子树\n",
    "                childnode.set_branchvalue(value)\n",
    "                print(childnode.branchvaule,len(X_subset),\"是否为好瓜:\",childnode.result)\n",
    "                node.set_child(childnode)#递归创建左子树\n",
    "    else:  # 处理连续属性\n",
    "            #for循环分别处理大于和小于两个子树\n",
    "        for num in range(2):\n",
    "            if num == 0:\n",
    "                #小于等于分割点的部分\n",
    "                mask = X[best_attribute] <= best_split_point\n",
    "                mask_valid = X_valid[best_attribute] <= best_split_point\n",
    "                X_subset = X[mask]\n",
    "                y_subset = y[mask]\n",
    "                X_valid_subset = X_valid[mask_valid]\n",
    "                y_valid_subset = y_valid[mask_valid]\n",
    "                if len(X_subset) == 0:  # 无样本，创建叶子节点，选择最多的类别作为结果\n",
    "                    leaf_node = DecisionTreeNode()\n",
    "                    most_common_label = y.value_counts().idxmax()\n",
    "                    leaf_node.set_result(most_common_label)\n",
    "                    leaf_node.set_branchvalue('<='+str(best_split_point))\n",
    "                    print(leaf_node.branchvaule)\n",
    "                    node.set_child(leaf_node)\n",
    "                    print('无样本，判断为',most_common_label)\n",
    "                else:\n",
    "                    print(node.attribute,'小于等于',node.split_point,'样本个数：',len(X_subset),'验证集样本个数：',len(X_valid_subset))\n",
    "                    childnode = build_decision_tree_prun(X_subset, y_subset,X_valid_subset,y_valid_subset)\n",
    "                    childnode.set_branchvalue('<='+str(best_split_point))\n",
    "                    print(childnode.branchvaule)\n",
    "                    node.set_child(childnode)\n",
    "            if num == 1:\n",
    "                #大于分割点的部分\n",
    "                mask = X[best_attribute] > best_split_point\n",
    "                mask_valid = X_valid[best_attribute] > best_split_point\n",
    "                X_subset = X[mask]\n",
    "                y_subset = y[mask]    \n",
    "                if len(X_subset) == 0:  # 无样本，创建叶子节点，选择最多的类别作为结果\n",
    "                    leaf_node = DecisionTreeNode()\n",
    "                    most_common_label = y.value_counts().idxmax()\n",
    "                    leaf_node.set_result(most_common_label)\n",
    "                    leaf_node.set_branchvalue('>'+str(best_split_point))\n",
    "                    print(leaf_node.branchvaule)\n",
    "                    node.set_child(leaf_node)\n",
    "                    print('无样本，判断为',most_common_label)\n",
    "                else:\n",
    "                    print(node.attribute,'大于',node.split_point,'样本个数：',len(X_subset),'验证集样本个数：',len(X_valid_subset))\n",
    "                    childnode = build_decision_tree_prun(X_subset, y_subset,X_valid_subset,y_valid_subset)\n",
    "                    childnode.set_branchvalue('>'+str(best_split_point))\n",
    "                    print(childnode.branchvaule)\n",
    "                    node.set_child(childnode)\n",
    "    return node\n",
    "\n",
    "\n",
    "#信息增益率预剪枝决策树代码\n",
    "def build_decision_tree_prun_rate(X, y, X_valid, y_valid):\n",
    "    global label_count \n",
    "    if len(set(y)) == 1:  # 所有样本属于同一类别，创建叶子节点\n",
    "        leaf_node = DecisionTreeNode()\n",
    "        leaf_node.set_result(y.iloc[0])\n",
    "        return leaf_node\n",
    "    \n",
    "    if len(X.columns) == 0:  # 所有属性已经用完，创建叶子节点，选择最多的类别作为结果\n",
    "        leaf_node = DecisionTreeNode()\n",
    "        most_common_label = y.value_counts().idxmax()\n",
    "        leaf_node.set_result(most_common_label)\n",
    "        return leaf_node\n",
    "\n",
    "    \n",
    "    \n",
    "     # 选取最佳划分属性\n",
    "    best_info_gain_rate = -np.inf\n",
    "    best_attribute = None\n",
    "    best_split_point = None\n",
    "    best_attribute,best_split_point,best_info_gain_rate = find_best_attribute2(X, y)\n",
    "    if best_info_gain_rate < 0:  # 信息增益小于0，创建叶子节点，选择最多的类别作为结果\n",
    "        leaf_node = DecisionTreeNode()\n",
    "        most_common_label = y.value_counts().idxmax()\n",
    "        leaf_node.set_result(most_common_label)\n",
    "        return leaf_node\n",
    "\n",
    "    \n",
    "    \n",
    "    node = DecisionTreeNode()\n",
    "    node.set_attribute(best_attribute)\n",
    "    node.set_split_point(best_split_point)\n",
    "    node.add_count()\n",
    "    node.set_fake_result(y.value_counts().idxmax())\n",
    "\n",
    "    if not node.is_leaf():\n",
    "        label_count += 1\n",
    "    \n",
    "    \n",
    "    #预剪枝,验证集不为空且所有样本不属于同一类别\n",
    "    if(is_pre_pruning(X,y,X_valid, y_valid,best_attribute,best_split_point,node)):\n",
    "        #剪枝\n",
    "        print(\"剪枝\")\n",
    "        most_common_label = y.value_counts().idxmax()#返回最多的类别\n",
    "        node.set_result(most_common_label)#设置叶子节点的结果\n",
    "        return node\n",
    "    else:\n",
    "        #不剪枝\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # 递归构建子树\n",
    "    if best_split_point is None:  # 处理离散属性\n",
    "        for value in X[best_attribute].unique():\n",
    "            mask = X[best_attribute] == value# 选出X中等于value的样本划分子树\n",
    "            mask_valid = X_valid[best_attribute] == value\n",
    "            X_subset = X[mask]\n",
    "            y_subset = y[mask]\n",
    "            X_valid_subset = X_valid[mask_valid]\n",
    "            y_valid_subset = y_valid[mask_valid]\n",
    "            print(X_subset,y_subset)\n",
    "            if len(X_subset) == 0:  # 无样本，创建叶子节点，选择最多的类别作为结果\n",
    "                leaf_node = DecisionTreeNode()\n",
    "                most_common_label = y.value_counts().idxmax()#返回最多的类别\n",
    "                leaf_node.set_result(most_common_label)#设置叶子节点的结果\n",
    "                leaf_node.set_branchvalue(value)\n",
    "                node.set_child(leaf_node)#设置左子树\n",
    "            else:\n",
    "                print(X_valid_subset,y_valid_subset)\n",
    "                childnode = build_decision_tree_prun(X_subset, y_subset,X_valid_subset,y_valid_subset)#递归创建左子树\n",
    "                childnode.set_branchvalue(value)\n",
    "                print(childnode.branchvaule,len(X_subset),\"是否为好瓜:\",childnode.result)\n",
    "                node.set_child(childnode)#递归创建左子树\n",
    "    else:  # 处理连续属性\n",
    "            #for循环分别处理大于和小于两个子树\n",
    "        for num in range(2):\n",
    "            if num == 0:\n",
    "                #小于等于分割点的部分\n",
    "                mask = X[best_attribute] <= best_split_point\n",
    "                mask_valid = X_valid[best_attribute] <= best_split_point\n",
    "                X_subset = X[mask]\n",
    "                y_subset = y[mask]\n",
    "                X_valid_subset = X_valid[mask_valid]\n",
    "                y_valid_subset = y_valid[mask_valid]\n",
    "                if len(X_subset) == 0:  # 无样本，创建叶子节点，选择最多的类别作为结果\n",
    "                    leaf_node = DecisionTreeNode()\n",
    "                    most_common_label = y.value_counts().idxmax()\n",
    "                    leaf_node.set_result(most_common_label)\n",
    "                    leaf_node.set_branchvalue('<='+str(best_split_point))\n",
    "                    print(leaf_node.branchvaule)\n",
    "                    node.set_child(leaf_node)\n",
    "                    print('无样本，判断为',most_common_label)\n",
    "                else:\n",
    "                    print(node.attribute,'小于等于',node.split_point,'样本个数：',len(X_subset),'验证集样本个数：',len(X_valid_subset))\n",
    "                    childnode = build_decision_tree_prun(X_subset, y_subset,X_valid_subset,y_valid_subset)\n",
    "                    childnode.set_branchvalue('<='+str(best_split_point))\n",
    "                    print(childnode.branchvaule)\n",
    "                    node.set_child(childnode)\n",
    "            if num == 1:\n",
    "                #大于分割点的部分\n",
    "                mask = X[best_attribute] > best_split_point\n",
    "                mask_valid = X_valid[best_attribute] > best_split_point\n",
    "                X_subset = X[mask]\n",
    "                y_subset = y[mask]    \n",
    "                if len(X_subset) == 0:  # 无样本，创建叶子节点，选择最多的类别作为结果\n",
    "                    leaf_node = DecisionTreeNode()\n",
    "                    most_common_label = y.value_counts().idxmax()\n",
    "                    leaf_node.set_result(most_common_label)\n",
    "                    leaf_node.set_branchvalue('>'+str(best_split_point))\n",
    "                    print(leaf_node.branchvaule)\n",
    "                    node.set_child(leaf_node)\n",
    "                    print('无样本，判断为',most_common_label)\n",
    "                else:\n",
    "                    print(node.attribute,'大于',node.split_point,'样本个数：',len(X_subset),'验证集样本个数：',len(X_valid_subset))\n",
    "                    childnode = build_decision_tree_prun(X_subset, y_subset,X_valid_subset,y_valid_subset)\n",
    "                    childnode.set_branchvalue('>'+str(best_split_point))\n",
    "                    print(childnode.branchvaule)\n",
    "                    node.set_child(childnode)\n",
    "    return node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后剪枝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#后剪枝\n",
    "\n",
    "#预测函数\n",
    "def predict_tree_single(node, x):\n",
    "    if node.is_leaf():\n",
    "        return node.result\n",
    "    else:\n",
    "        #如果是连续值属性\n",
    "        if node.split_point is not None:\n",
    "            if x[node.attribute] <= node.split_point:\n",
    "                return predict_tree_single(node.children[0], x)\n",
    "            else:\n",
    "                return predict_tree_single(node.children[1], x)\n",
    "        else:#离散值属性\n",
    "            for child in node.children:\n",
    "                if child.branchvaule == x[node.attribute]:\n",
    "                    return predict_tree_single(child, x)\n",
    "                \n",
    "#生成预测结果\n",
    "def generate_post_y(decision_tree, X):\n",
    "    y_pred = []\n",
    "    for i in range(len(X)):\n",
    "        x = X.iloc[i]\n",
    "        y_pred.append(predict_tree_single(decision_tree, x))\n",
    "    return np.array(y_pred)\n",
    "\n",
    "\n",
    "def calculate_accuracy(node, X_val, y_val):\n",
    "    # 使用从给定节点开始的树进行预测\n",
    "    predictions = generate_post_y(node, X_val)\n",
    "\n",
    "    # 计算准确性\n",
    "    correct_predictions = sum(1 for p, true_label in zip(predictions, y_val) if p == true_label)\n",
    "    return correct_predictions / len(y_val)\n",
    "\n",
    "\n",
    "def find_max_label_nodes(node):\n",
    "    # 广度优先搜索找到具有最大label值的节点\n",
    "    max_label_nodes = []\n",
    "\n",
    "    queue = [node]\n",
    "    while queue:\n",
    "        current_node = queue.pop(0)\n",
    "        if not current_node.is_leaf():\n",
    "            max_label_nodes.append(current_node)\n",
    "\n",
    "        queue.extend(child for child in current_node.children if not child.is_leaf())\n",
    "\n",
    "    # 按label值从大到小排序\n",
    "    max_label_nodes.sort(key=lambda x: x.label, reverse=True)\n",
    "\n",
    "    return max_label_nodes\n",
    "\n",
    "def post_pruning(decisiontree, X_val, y_val):\n",
    "    processed_nodes = set()\n",
    "\n",
    "    max_label_nodes = find_max_label_nodes(decisiontree)\n",
    "\n",
    "    for node in max_label_nodes:\n",
    "        if node in processed_nodes:\n",
    "            continue\n",
    "\n",
    "        # 计算剪枝前的验证集准确性\n",
    "        accuracy_before = calculate_accuracy(decisiontree, X_val, y_val)\n",
    "        print(\"剪枝前的准确率：\", accuracy_before)\n",
    "\n",
    "        # 尝试剪枝\n",
    "        node.result = node.fake_result\n",
    "\n",
    "        # 计算剪枝后的验证集准确性\n",
    "        accuracy_after = calculate_accuracy(decisiontree, X_val, y_val)\n",
    "        print(\"假设剪枝后的准确率：\", accuracy_after)\n",
    "\n",
    "        # 恢复原始结果值\n",
    "        node.result = None\n",
    "\n",
    "        # 比较准确性并进行剪枝\n",
    "        if accuracy_after >= accuracy_before:\n",
    "            print(\"剪枝, 该节点label值为：\", node.label)\n",
    "            node.set_result(node.fake_result)  # 剪枝\n",
    "            node.set_attribute(None)\n",
    "            processed_nodes.add(node)\n",
    "        else:\n",
    "            print(\"不剪枝, 该节点label值为：\", node.label)\n",
    "            processed_nodes.add(node)\n",
    "\n",
    "# 使用示例\n",
    "# decisiontree 是树的根节点\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import graphviz\n",
    "dot = graphviz.Digraph()\n",
    "\n",
    "\n",
    "parent = None\n",
    "leaf_counter = 0\n",
    "\n",
    "def add_nodes(node):\n",
    "    global parent\n",
    "    global leaf_counter\n",
    "    if node.is_leaf():\n",
    "        leaf_name = str(node.result + str(leaf_counter))\n",
    "        dot.node(name=leaf_name,fontname =\"SimHei\") \n",
    "        dot.edge(tail_name=parent, head_name=leaf_name, label=str(node.branchvaule),fontname =\"SimHei\")\n",
    "        leaf_counter += 1\n",
    "        return\n",
    "    else:\n",
    "        dot.node(name=str(node.attribute),fontname =\"SimHei\")\n",
    "        for child in node.children:\n",
    "            parent = node.attribute \n",
    "            #如果孩子是非叶子节点\n",
    "            if not child.is_leaf():\n",
    "                dot.edge(tail_name=parent, head_name=str(child.attribute), label=str(child.branchvaule),fontname =\"SimHei\")\n",
    "                add_nodes(child)\n",
    "            if child.is_leaf():\n",
    "                leaf_name = str(child.result + str(leaf_counter))\n",
    "                dot.node(name=leaf_name,fontname =\"SimHei\") \n",
    "                dot.edge(tail_name=parent, head_name=leaf_name, label=str(child.branchvaule),fontname =\"SimHei\")\n",
    "                leaf_counter += 1\n",
    "            parent = node.attribute\n",
    "    leaf_counter = 0\n",
    "    return\n",
    "# add_nodes(decision_tree)\n",
    "# dot.render('decision_tree',format='png')\n",
    "# dot.clear()\n",
    "# post_pruning(decision_tree, X_test, y_test)\n",
    "# add_nodes(decision_tree)\n",
    "# dot.render('decision_tree_postpruning',format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准确率比较:信息增益树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    色泽  根蒂  敲声  纹理  脐部  触感     密度    含糖率\n",
      "12  青绿  稍蜷  浊响  稍糊  凹陷  硬滑  0.639  0.161\n",
      "14  乌黑  稍蜷  浊响  清晰  稍凹  软粘  0.360  0.370\n",
      "0   青绿  蜷缩  浊响  清晰  凹陷  硬滑  0.697  0.460\n",
      "10  浅白  硬挺  清脆  模糊  平坦  硬滑  0.245  0.057\n",
      "3   青绿  蜷缩  沉闷  清晰  凹陷  硬滑  0.608  0.318\n",
      "2   乌黑  蜷缩  浊响  清晰  凹陷  硬滑  0.634  0.264\n",
      "1   乌黑  蜷缩  沉闷  清晰  凹陷  硬滑  0.774  0.376\n",
      "13  浅白  稍蜷  沉闷  稍糊  凹陷  硬滑  0.657  0.198\n",
      "6   乌黑  稍蜷  浊响  稍糊  稍凹  软粘  0.481  0.149\n",
      "16  青绿  蜷缩  沉闷  稍糊  稍凹  硬滑  0.719  0.103\n",
      "11  浅白  蜷缩  浊响  模糊  平坦  软粘  0.343  0.099 12    否\n",
      "14    否\n",
      "0     是\n",
      "10    否\n",
      "3     是\n",
      "2     是\n",
      "1     是\n",
      "13    否\n",
      "6     是\n",
      "16    否\n",
      "11    否\n",
      "Name: 好瓜, dtype: object\n",
      "当前最佳划分属性： 纹理 当前标签值： 0 当前假设结果： 否\n",
      "    色泽  根蒂  敲声  纹理  脐部  触感     密度    含糖率\n",
      "12  青绿  稍蜷  浊响  稍糊  凹陷  硬滑  0.639  0.161\n",
      "13  浅白  稍蜷  沉闷  稍糊  凹陷  硬滑  0.657  0.198\n",
      "6   乌黑  稍蜷  浊响  稍糊  稍凹  软粘  0.481  0.149\n",
      "16  青绿  蜷缩  沉闷  稍糊  稍凹  硬滑  0.719  0.103 12    否\n",
      "13    否\n",
      "6     是\n",
      "16    否\n",
      "Name: 好瓜, dtype: object\n",
      "当前最佳划分属性： 色泽 当前标签值： 1 当前假设结果： 否\n",
      "青绿 2 是否为好瓜: 否\n",
      "浅白 1 是否为好瓜: 否\n",
      "乌黑 1 是否为好瓜: 是\n",
      "稍糊 4 是否为好瓜: None\n",
      "    色泽  根蒂  敲声  纹理  脐部  触感     密度    含糖率\n",
      "14  乌黑  稍蜷  浊响  清晰  稍凹  软粘  0.360  0.370\n",
      "0   青绿  蜷缩  浊响  清晰  凹陷  硬滑  0.697  0.460\n",
      "3   青绿  蜷缩  沉闷  清晰  凹陷  硬滑  0.608  0.318\n",
      "2   乌黑  蜷缩  浊响  清晰  凹陷  硬滑  0.634  0.264\n",
      "1   乌黑  蜷缩  沉闷  清晰  凹陷  硬滑  0.774  0.376 14    否\n",
      "0     是\n",
      "3     是\n",
      "2     是\n",
      "1     是\n",
      "Name: 好瓜, dtype: object\n",
      "当前最佳划分属性： 根蒂 当前标签值： 2 当前假设结果： 是\n",
      "稍蜷 1 是否为好瓜: 否\n",
      "蜷缩 4 是否为好瓜: 是\n",
      "清晰 5 是否为好瓜: None\n",
      "模糊 2 是否为好瓜: 否\n",
      "未剪枝准确率: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "#未剪枝\n",
    "dot.clear()\n",
    "decision_tree = build_decision_tree(X_train, y_train)\n",
    "print('未剪枝准确率:',calculate_accuracy(decision_tree, X_test, y_test))\n",
    "add_nodes(decision_tree)\n",
    "dot.render('tree',format='png')\n",
    "dot.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "划分前的结果： 否\n",
      "划分前的准确率： 0.5\n",
      "['否' '是' '是' '是' '是' '否']\n",
      "离散属性划分后的准确率： 0.16666666666666666\n",
      "剪枝\n",
      "预剪枝准确率: 0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#预剪枝\n",
    "decision_tree_prun =  build_decision_tree_prun(X_train, y_train,X_test, y_test)\n",
    "print('预剪枝准确率:',calculate_accuracy(decision_tree_prun, X_test, y_test))\n",
    "add_nodes(decision_tree_prun)\n",
    "dot.render('tree_prun',format='png')\n",
    "dot.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剪枝前的准确率： 0.3333333333333333\n",
      "假设剪枝后的准确率： 0.6666666666666666\n",
      "剪枝, 该节点label值为： 2\n",
      "剪枝前的准确率： 0.6666666666666666\n",
      "假设剪枝后的准确率： 0.8333333333333334\n",
      "剪枝, 该节点label值为： 1\n",
      "剪枝前的准确率： 0.8333333333333334\n",
      "假设剪枝后的准确率： 0.5\n",
      "不剪枝, 该节点label值为： 0\n",
      "后剪枝准确率: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "#后剪枝\n",
    "post_pruning(decision_tree, X_test, y_test)\n",
    "print('后剪枝准确率:',calculate_accuracy(decision_tree, X_test, y_test))\n",
    "add_nodes(decision_tree)\n",
    "dot.render('tree_postpruning',format='png')\n",
    "dot.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未剪枝准确率: 0.8333333333333334\n",
      "预剪枝准确率: 0.5\n",
      "后剪枝准确率: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "print('未剪枝准确率:',calculate_accuracy(decision_tree, X_test, y_test))\n",
    "print('预剪枝准确率:',calculate_accuracy(decision_tree_prun, X_test, y_test))\n",
    "print('后剪枝准确率:',calculate_accuracy(decision_tree, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准确率比较:信息增益率树\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    色泽  根蒂  敲声  纹理  脐部  触感     密度    含糖率\n",
      "12  青绿  稍蜷  浊响  稍糊  凹陷  硬滑  0.639  0.161\n",
      "14  乌黑  稍蜷  浊响  清晰  稍凹  软粘  0.360  0.370\n",
      "0   青绿  蜷缩  浊响  清晰  凹陷  硬滑  0.697  0.460\n",
      "10  浅白  硬挺  清脆  模糊  平坦  硬滑  0.245  0.057\n",
      "3   青绿  蜷缩  沉闷  清晰  凹陷  硬滑  0.608  0.318\n",
      "2   乌黑  蜷缩  浊响  清晰  凹陷  硬滑  0.634  0.264\n",
      "1   乌黑  蜷缩  沉闷  清晰  凹陷  硬滑  0.774  0.376\n",
      "13  浅白  稍蜷  沉闷  稍糊  凹陷  硬滑  0.657  0.198\n",
      "6   乌黑  稍蜷  浊响  稍糊  稍凹  软粘  0.481  0.149\n",
      "16  青绿  蜷缩  沉闷  稍糊  稍凹  硬滑  0.719  0.103\n",
      "11  浅白  蜷缩  浊响  模糊  平坦  软粘  0.343  0.099 12    否\n",
      "14    否\n",
      "0     是\n",
      "10    否\n",
      "3     是\n",
      "2     是\n",
      "1     是\n",
      "13    否\n",
      "6     是\n",
      "16    否\n",
      "11    否\n",
      "Name: 好瓜, dtype: object\n",
      "当前最佳划分属性： 密度 当前标签值： 4 当前假设结果： 否\n",
      "密度 小于等于 0.4205 样本个数： 3\n",
      "<=0.4205 3 是否为好瓜: 否\n",
      "密度 大于 0.4205 样本个数： 8\n",
      "    色泽  根蒂  敲声  纹理  脐部  触感     密度    含糖率\n",
      "12  青绿  稍蜷  浊响  稍糊  凹陷  硬滑  0.639  0.161\n",
      "0   青绿  蜷缩  浊响  清晰  凹陷  硬滑  0.697  0.460\n",
      "3   青绿  蜷缩  沉闷  清晰  凹陷  硬滑  0.608  0.318\n",
      "2   乌黑  蜷缩  浊响  清晰  凹陷  硬滑  0.634  0.264\n",
      "1   乌黑  蜷缩  沉闷  清晰  凹陷  硬滑  0.774  0.376\n",
      "13  浅白  稍蜷  沉闷  稍糊  凹陷  硬滑  0.657  0.198\n",
      "6   乌黑  稍蜷  浊响  稍糊  稍凹  软粘  0.481  0.149\n",
      "16  青绿  蜷缩  沉闷  稍糊  稍凹  硬滑  0.719  0.103 12    否\n",
      "0     是\n",
      "3     是\n",
      "2     是\n",
      "1     是\n",
      "13    否\n",
      "6     是\n",
      "16    否\n",
      "Name: 好瓜, dtype: object\n",
      "当前最佳划分属性： 纹理 当前标签值： 5 当前假设结果： 是\n",
      "    色泽  根蒂  敲声  纹理  脐部  触感     密度    含糖率\n",
      "12  青绿  稍蜷  浊响  稍糊  凹陷  硬滑  0.639  0.161\n",
      "13  浅白  稍蜷  沉闷  稍糊  凹陷  硬滑  0.657  0.198\n",
      "6   乌黑  稍蜷  浊响  稍糊  稍凹  软粘  0.481  0.149\n",
      "16  青绿  蜷缩  沉闷  稍糊  稍凹  硬滑  0.719  0.103 12    否\n",
      "13    否\n",
      "6     是\n",
      "16    否\n",
      "Name: 好瓜, dtype: object\n",
      "当前最佳划分属性： 色泽 当前标签值： 6 当前假设结果： 否\n",
      "青绿 2 是否为好瓜: 否\n",
      "浅白 1 是否为好瓜: 否\n",
      "乌黑 1 是否为好瓜: 是\n",
      "稍糊 4 是否为好瓜: None\n",
      "清晰 4 是否为好瓜: 是\n",
      ">0.4205 8 是否为好瓜: None\n",
      "未剪枝准确率: 0.5\n"
     ]
    }
   ],
   "source": [
    "#未剪枝\n",
    "dot.clear()\n",
    "decision_tree_rate = build_decision_tree_rate(X_train, y_train)\n",
    "print('未剪枝准确率:',calculate_accuracy(decision_tree_rate, X_test, y_test))\n",
    "add_nodes(decision_tree_rate)\n",
    "dot.render('rate_tree',format='png')\n",
    "dot.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "划分前的结果： 否\n",
      "划分前的准确率： 0.5\n",
      "['否' '否' '是' '是' '是' '是']\n",
      "连续属性划分后的准确率： 0.5\n",
      "剪枝\n",
      "预剪枝准确率: 0.5\n"
     ]
    }
   ],
   "source": [
    "#预剪枝\n",
    "decision_tree_prun_rate =  build_decision_tree_prun_rate(X_train, y_train,X_test, y_test)\n",
    "print('预剪枝准确率:',calculate_accuracy(decision_tree_prun_rate, X_test, y_test))\n",
    "add_nodes(decision_tree_prun_rate)\n",
    "dot.render('rate_tree_prun',format='png')\n",
    "dot.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剪枝前的准确率： 0.5\n",
      "假设剪枝后的准确率： 0.6666666666666666\n",
      "剪枝, 该节点label值为： 6\n",
      "剪枝前的准确率： 0.6666666666666666\n",
      "假设剪枝后的准确率： 0.5\n",
      "不剪枝, 该节点label值为： 5\n",
      "剪枝前的准确率： 0.6666666666666666\n",
      "假设剪枝后的准确率： 0.5\n",
      "不剪枝, 该节点label值为： 4\n",
      "后剪枝准确率: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "#后剪枝\n",
    "post_pruning(decision_tree_rate, X_test, y_test)\n",
    "print('后剪枝准确率:',calculate_accuracy(decision_tree, X_test, y_test))\n",
    "add_nodes(decision_tree_rate)\n",
    "dot.render('rate_tree_postpruning',format='png')\n",
    "dot.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未剪枝准确率: 0.6666666666666666\n",
      "预剪枝准确率: 0.5\n",
      "后剪枝准确率: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print('未剪枝准确率:',calculate_accuracy(decision_tree_rate, X_test, y_test))\n",
    "print('预剪枝准确率:',calculate_accuracy(decision_tree_prun_rate, X_test, y_test))\n",
    "print('后剪枝准确率:',calculate_accuracy(decision_tree_rate, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('penguins.csv') \n",
    "\n",
    "X = data.iloc[:,1:-1]  \n",
    "y = data.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "# print(X_train)\n",
    "# print(y_train)\n",
    "# print(X_test)\n",
    "# print(y_test)\n",
    "#打印每种属性的种类\n",
    "# for col in X.columns:\n",
    "#     print(col,\":\",col.type)\n",
    "\n",
    "def build_decision_tree_penguins(X, y):\n",
    "    global label_count \n",
    "    if len(set(y)) == 1:  # 所有样本属于同一类别，创建叶子节点\n",
    "        leaf_node = DecisionTreeNode()\n",
    "        leaf_node.set_result(y.iloc[0])\n",
    "        return leaf_node\n",
    "    \n",
    "    if len(X.columns) == 0:  # 所有属性已经用完，创建叶子节点，选择最多的类别作为结果\n",
    "        leaf_node = DecisionTreeNode()\n",
    "        most_common_label = y.value_counts().idxmax()\n",
    "        leaf_node.set_result(most_common_label)\n",
    "        return leaf_node\n",
    "    \n",
    "    # 选取最佳划分属性\n",
    "    best_info_gain = -np.inf\n",
    "    best_attribute = None\n",
    "    best_split_point = None\n",
    "    best_attribute,best_split_point,best_info_gain = find_best_attribute(X, y)\n",
    "    if best_info_gain < 0:  # 信息增益小于0，创建叶子节点，选择最多的类别作为结果\n",
    "        leaf_node = DecisionTreeNode()\n",
    "        most_common_label = y.value_counts().idxmax()\n",
    "        leaf_node.set_result(most_common_label)\n",
    "        return leaf_node\n",
    "    \n",
    "    node = DecisionTreeNode()\n",
    "    node.set_attribute(best_attribute)\n",
    "    node.set_split_point(best_split_point)\n",
    "    node.add_count()\n",
    "    node.set_fake_result(y.value_counts().idxmax())\n",
    "    \n",
    "    if not node.is_leaf():\n",
    "        label_count += 1\n",
    "        \n",
    "    print(X,y)\n",
    "    print(\"当前最佳划分属性：\",node.attribute,\"当前标签值：\",node.label,\"当前假设结果：\",node.fake_result)\n",
    "    \n",
    "    # 递归构建子树\n",
    "    if best_split_point is None:  # 处理离散属性\n",
    "        for value in X[best_attribute].unique():\n",
    "            mask = X[best_attribute] == value# 选出X中等于value的样本划分子树\n",
    "            X_subset = X[mask]\n",
    "            y_subset = y[mask]\n",
    "            #分完后将该属性从X中删除\n",
    "            X_subset = X_subset.drop(columns=[best_attribute])\n",
    "            if len(X_subset) == 0:  # 无样本，创建叶子节点，选择最多的类别作为结果\n",
    "                leaf_node = DecisionTreeNode()\n",
    "                most_common_label = y.value_counts().idxmax()#返回最多的类别\n",
    "                leaf_node.set_result(most_common_label)#设置叶子节点的结果\n",
    "                leaf_node.set_branchvalue(value)\n",
    "                print(leaf_node.branchvaule,0)\n",
    "                node.set_child(leaf_node)#设置子树\n",
    "            else:\n",
    "                childnode = build_decision_tree(X_subset, y_subset)#递归创建左子树\n",
    "                childnode.set_branchvalue(value)\n",
    "                print(childnode.branchvaule,len(X_subset),\"是否为好瓜:\",childnode.result)\n",
    "                node.set_child(childnode)#递归创建子树\n",
    "    else:  # 处理连续属性\n",
    "            #for循环分别处理大于和小于两个子树\n",
    "        for num in range(2):\n",
    "            if num == 0:\n",
    "                #小于等于分割点的部分\n",
    "                mask = X[best_attribute] <= best_split_point\n",
    "                X_subset = X[mask]\n",
    "                y_subset = y[mask]\n",
    "                #分完后将该属性从X中删除\n",
    "                if len(X_subset) == 0:  # 无样本，创建叶子节点，选择最多的类别作为结果\n",
    "                    leaf_node = DecisionTreeNode()\n",
    "                    most_common_label = y.value_counts().idxmax()\n",
    "                    leaf_node.set_result(most_common_label)\n",
    "                    leaf_node.set_branchvalue('<='+str(best_split_point))\n",
    "                    # print(leaf_node.branchvaule)\n",
    "                    node.set_child(leaf_node)\n",
    "                    print('无样本，判断为',most_common_label)\n",
    "                else:\n",
    "                    X_subset = X_subset.drop(columns=[best_attribute])\n",
    "                    print(node.attribute,'小于等于',node.split_point,'样本个数：',len(X_subset))\n",
    "                    childnode = build_decision_tree(X_subset, y_subset)\n",
    "                    childnode.set_branchvalue('<='+str(best_split_point))\n",
    "                    # print(childnode.branchvaule,len(X_subset),\"是否为好瓜:\",childnode.result)\n",
    "                    node.set_child(childnode)\n",
    "            if num == 1:\n",
    "                #大于分割点的部分\n",
    "                mask = X[best_attribute] > best_split_point\n",
    "                X_subset = X[mask]\n",
    "                y_subset = y[mask]\n",
    "                if len(X_subset) == 0:  # 无样本，创建叶子节点，选择最多的类别作为结果\n",
    "                    leaf_node = DecisionTreeNode()\n",
    "                    most_common_label = y.value_counts().idxmax()\n",
    "                    leaf_node.set_result(most_common_label)\n",
    "                    leaf_node.set_branchvalue('>'+str(best_split_point))\n",
    "                    print(leaf_node.branchvaule,leaf_node.result)\n",
    "                    node.set_child(leaf_node)\n",
    "                    print('无样本，判断为',most_common_label)\n",
    "                else:\n",
    "                    print(node.attribute,'大于',node.split_point,'样本个数：',len(X_subset))\n",
    "                    X_subset = X_subset.drop(columns=[best_attribute])\n",
    "                    childnode = build_decision_tree(X_subset, y_subset)\n",
    "                    childnode.set_branchvalue('>'+str(best_split_point))\n",
    "                    # print(childnode.branchvaule,len(X_subset),\"是否为好瓜:\",childnode.result)\n",
    "                    node.set_child(childnode)\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g     sex  \\\n",
      "292           331.1          310.7              473.0       4042.0  female   \n",
      "298           337.1          316.7              479.0       4048.0  female   \n",
      "106           145.1          124.7              287.0       3856.0    male   \n",
      "47             86.1           65.7              228.0       3797.0  female   \n",
      "77            116.1           95.7              258.0       3827.0  female   \n",
      "..              ...            ...                ...          ...     ...   \n",
      "130           169.1          148.7              311.0       3880.0    male   \n",
      "241           280.1          259.7              422.0       3991.0  female   \n",
      "253           292.1          271.7              434.0       4003.0  female   \n",
      "155           194.1          173.7              336.0       3905.0    male   \n",
      "331           370.1          349.7              512.0       4081.0    male   \n",
      "\n",
      "     year  \n",
      "292  2007  \n",
      "298  2007  \n",
      "106  2009  \n",
      "47   2007  \n",
      "77   2008  \n",
      "..    ...  \n",
      "130  2009  \n",
      "241  2009  \n",
      "253  2009  \n",
      "155  2007  \n",
      "331  2009  \n",
      "\n",
      "[239 rows x 6 columns] 292    Chinstrap\n",
      "298    Chinstrap\n",
      "106       Adelie\n",
      "47        Adelie\n",
      "77        Adelie\n",
      "         ...    \n",
      "130       Adelie\n",
      "241       Gentoo\n",
      "253       Gentoo\n",
      "155       Gentoo\n",
      "331    Chinstrap\n",
      "Name: species, Length: 239, dtype: object\n",
      "当前最佳划分属性： bill_length_mm 当前标签值： 0 当前假设结果： Adelie\n",
      "bill_length_mm 小于等于 190.1 样本个数： 105\n",
      "bill_length_mm 大于 190.1 样本个数： 134\n",
      "     bill_depth_mm  flipper_length_mm  body_mass_g     sex  year\n",
      "292          310.7              473.0       4042.0  female  2007\n",
      "298          316.7              479.0       4048.0  female  2007\n",
      "314          332.7              495.0       4064.0    male  2008\n",
      "315          333.7              496.0       4065.0  female  2008\n",
      "202          220.7              383.0       3952.0    male  2008\n",
      "..             ...                ...          ...     ...   ...\n",
      "259          277.7              440.0       4009.0  female  2009\n",
      "241          259.7              422.0       3991.0  female  2009\n",
      "253          271.7              434.0       4003.0  female  2009\n",
      "155          173.7              336.0       3905.0    male  2007\n",
      "331          349.7              512.0       4081.0    male  2009\n",
      "\n",
      "[134 rows x 5 columns] 292    Chinstrap\n",
      "298    Chinstrap\n",
      "314    Chinstrap\n",
      "315    Chinstrap\n",
      "202       Gentoo\n",
      "         ...    \n",
      "259       Gentoo\n",
      "241       Gentoo\n",
      "253       Gentoo\n",
      "155       Gentoo\n",
      "331    Chinstrap\n",
      "Name: species, Length: 134, dtype: object\n",
      "当前最佳划分属性： bill_depth_mm 当前标签值： 1 当前假设结果： Gentoo\n",
      "bill_depth_mm 小于等于 292.2 样本个数： 94\n",
      "<=292.2 94 是否为好瓜: Gentoo\n",
      "bill_depth_mm 大于 292.2 样本个数： 40\n",
      ">292.2 40 是否为好瓜: Chinstrap\n",
      "未剪枝准确率: 0.9902912621359223\n"
     ]
    }
   ],
   "source": [
    "label_count = 0\n",
    "penguins_tree = build_decision_tree_penguins(X_train, y_train)\n",
    "\n",
    "print('未剪枝准确率:',calculate_accuracy(penguins_tree, X_test, y_test))\n",
    "add_nodes(penguins_tree)\n",
    "dot.render('penguins_tree',format='png')\n",
    "dot.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3810",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
